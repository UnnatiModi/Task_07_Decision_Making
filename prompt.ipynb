{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f11d689-87ac-4609-bff1-95465714309d",
   "metadata": {},
   "source": [
    " Basic Factual Questions\n",
    "\n",
    "Q1: What team had the highest number of wins between 2003 and 2023?  \n",
    "A:New England Patriots  \n",
    "Accuracy:Verified correct.\n",
    "\n",
    "Q2:In which year was the average points per team the highest?  \n",
    "A:2013  \n",
    "Accuracy:Incorrect — actual highest was 2018.\n",
    "\n",
    "Insightful Questions\n",
    "\n",
    "Q1:Is there a correlation between points scored and win percentage?  \n",
    "A:Yes, teams with higher points generally have higher win percentages.  \n",
    "Accuracy:Supported by visualization.\n",
    "\n",
    "Q2:Are turnovers a significant factor in determining wins?  \n",
    "A:Yes, teams with fewer turnovers tend to win more games.  \n",
    "Accuracy:Verified through scatterplot.\n",
    "\n",
    "Comparative Questions\n",
    "\n",
    "Q1:Which team had more total wins — Patriots or Steelers?  \n",
    "A:Patriots  \n",
    "Accuracy:Confirmed by dataset.\n",
    "\n",
    "Q2:Do NFC teams score more points than AFC teams on average?  \n",
    "A:Yes.  \n",
    "Accuracy:This wasn't verified in current analysis.\n",
    "\n",
    "Reasoning-Based Questions\n",
    "\n",
    "Q1:Could high-scoring teams still lose frequently?  \n",
    "A:Yes, if their defense allows too many points.  \n",
    "Accuracy:Theoretically valid, but needs points allowed data.\n",
    "\n",
    "Q2:Does consistency in yards per play guarantee more wins?  \n",
    "A:Possibly, but it needs to be combined with low turnovers and strong defense.  \n",
    "Accuracy:Reasonable assumption.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ec389c4c-0e9a-4ffc-a3bd-a322a79623ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧠 LLM received: Which team had the highest passing yards in 2022?\n",
      "This is a placeholder LLM response. You should fill in real reasoning.\n"
     ]
    }
   ],
   "source": [
    "def ask_llm(prompt: str) -> str:\n",
    "    print(f\"🧠 LLM received: {prompt}\")\n",
    "    return \"This is a placeholder LLM response. You should fill in real reasoning.\"\n",
    "\n",
    "# Then call:\n",
    "question = \"Which team had the highest passing yards in 2022?\"\n",
    "response = ask_llm(question)\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "551ec2ef-7e59-46e7-9bdc-c1a0ce42d3fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧠 LLM received: Which team had the most wins in 2022?\n",
      "This is a placeholder LLM response. You should fill in real reasoning.\n",
      "🧠 LLM received: Did teams with higher offensive yards generally win more games in 2022?\n",
      "This is a placeholder LLM response. You should fill in real reasoning.\n",
      "🧠 LLM received: Is it accurate to assume low turnovers result in more wins?\n",
      "This is a placeholder LLM response. You should fill in real reasoning.\n",
      "🧠 LLM received: Which team improved the most from 2021 to 2022 in terms of total yards?\n",
      "This is a placeholder LLM response. You should fill in real reasoning.\n",
      "🧠 LLM received: Were there teams that had high yardage but low win counts in 2022?\n",
      "This is a placeholder LLM response. You should fill in real reasoning.\n"
     ]
    }
   ],
   "source": [
    "# Prompt 1: Factual\n",
    "question_1 = \"Which team had the most wins in 2022?\"\n",
    "response_1 = ask_llm(question_1)\n",
    "print(response_1)\n",
    "\n",
    "# Prompt 2: Comparative\n",
    "question_2 = \"Did teams with higher offensive yards generally win more games in 2022?\"\n",
    "response_2 = ask_llm(question_2)\n",
    "print(response_2)\n",
    "\n",
    "# Prompt 3: Reasoning\n",
    "question_3 = \"Is it accurate to assume low turnovers result in more wins?\"\n",
    "response_3 = ask_llm(question_3)\n",
    "print(response_3)\n",
    "\n",
    "# Prompt 4: Trend-based\n",
    "question_4 = \"Which team improved the most from 2021 to 2022 in terms of total yards?\"\n",
    "response_4 = ask_llm(question_4)\n",
    "print(response_4)\n",
    "\n",
    "# Prompt 5: Anomalies\n",
    "question_5 = \"Were there teams that had high yardage but low win counts in 2022?\"\n",
    "response_5 = ask_llm(question_5)\n",
    "print(response_5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec3e7b00-5140-4e5c-b33a-c3bd887b5990",
   "metadata": {},
   "source": [
    "### 🔍 Evaluation of LLM Responses\n",
    "\n",
    "#### Prompt 1: \"Which team had the most wins in 2022?\"\n",
    "- **LLM Response:** Placeholder\n",
    "- **Accuracy:** Needs verification via dataset\n",
    "- **Fact-Check Method:** Filter `year == 2022`, sort by `wins`\n",
    "- **Reproducibility:** ✅ Easy to verify\n",
    "- **Fairness/Risk:** Low — safe factual question\n",
    "\n",
    "---\n",
    "\n",
    "#### Prompt 2: \"Did teams with higher offensive yards generally win more games in 2022?\"\n",
    "- **LLM Response:** Placeholder\n",
    "- **Accuracy:** Requires correlation analysis\n",
    "- **Fact-Check Method:** Scatter plot `total_off_yards` vs `wins`\n",
    "- **Insight:** Expect positive correlation\n",
    "- **\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dfd1f30f-7dff-42ca-b21f-cb97cf967458",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧠 LLM received: Which team had the most rushing touchdowns in 2020?\n",
      "This is a placeholder LLM response. You should fill in real reasoning.\n",
      "🧠 LLM received: What is the average number of points scored per season between 2010 and 2020?\n",
      "This is a placeholder LLM response. You should fill in real reasoning.\n",
      "🧠 LLM received: Is there a strong correlation between total offensive yards and wins?\n",
      "This is a placeholder LLM response. You should fill in real reasoning.\n",
      "🧠 LLM received: Based on historical trends, what type of teams tend to reach the playoffs?\n",
      "This is a placeholder LLM response. You should fill in real reasoning.\n"
     ]
    }
   ],
   "source": [
    "# Prompt 2: Factual question\n",
    "question_2 = \"Which team had the most rushing touchdowns in 2020?\"\n",
    "response_2 = ask_llm(question_2)\n",
    "print(response_2)\n",
    "\n",
    "# Prompt 3: Descriptive statistics question\n",
    "question_3 = \"What is the average number of points scored per season between 2010 and 2020?\"\n",
    "response_3 = ask_llm(question_3)\n",
    "print(response_3)\n",
    "\n",
    "# Prompt 4: Reasoning question\n",
    "question_4 = \"Is there a strong correlation between total offensive yards and wins?\"\n",
    "response_4 = ask_llm(question_4)\n",
    "print(response_4)\n",
    "\n",
    "# Prompt 5: Insightful question\n",
    "question_5 = \"Based on historical trends, what type of teams tend to reach the playoffs?\"\n",
    "response_5 = ask_llm(question_5)\n",
    "print(response_5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eea38e9b-da24-4160-803b-1e40daa964e8",
   "metadata": {},
   "source": [
    "## 🧠 LLM Prompt Reflections\n",
    "\n",
    "We tested the placeholder LLM with a range of questions:\n",
    "- **Factual (e.g., top passing team)**\n",
    "- **Statistical (e.g., average points)**\n",
    "- **Correlational (e.g., yards vs. wins)**\n",
    "- **Insightful (e.g., playoff trends)**\n",
    "\n",
    "These help assess reasoning ability, accuracy, and interpretability.\n",
    "\n",
    "### Observations:\n",
    "- **Factual answers** can be verified from EDA/stats.\n",
    "- **Reasoning questions** like correlation or prediction show potential for pattern recognition.\n",
    "- **Insightful queries** reflect how LLMs can assist in hypothesis generation.\n",
    "\n",
    "### Next:\n",
    "We will evaluate these prompts using criteria like **correctness**, **relevance**, and **explanation quality** in the final report.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d720203e-56a0-4a01-ac24-4a8dbf19dafd",
   "metadata": {},
   "source": [
    "## ✅ Prompt Evaluation\n",
    "\n",
    "We assessed the LLM’s responses across different types of prompts using three criteria:\n",
    "\n",
    "| Prompt Type      | Prompt | LLM Response | Correct? | Relevant? | Explanation Quality |\n",
    "|------------------|--------|---------------|----------|-----------|----------------------|\n",
    "| Factual          | Which team had the highest passing yards in 2022? | [Your LLM output here] | ✅ / ❌ | ✅ / ❌ | Strong / Weak |\n",
    "| Reasoning        | Is there a correlation between total yards and wins? | [LLM output] | ✅ / ❌ | ✅ / ❌ | Strong / Weak |\n",
    "| Insightful       | Do teams with more passing yards tend to win more? | [LLM output] | ✅ / ❌ | ✅ / ❌ | Strong / Weak |\n",
    "\n",
    "> 💡 **Tip:** Fill in the real LLM outputs and your judgment of their accuracy and depth.\n",
    "\n",
    "---\n",
    "\n",
    "### Summary:\n",
    "- **Correctness**: Was the response factually correct?\n",
    "- **Relevance**: Was the response related to the dataset context?\n",
    "- **Explanation**: Did the LLM provide reasoning or just a guess?\n",
    "\n",
    "This evaluation helps reflect **how useful the LLM is for data-driven analysis**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e64807f-2bd6-427e-8008-310db0b5ed88",
   "metadata": {},
   "source": [
    "Conclusion: Can LLMs Help with Data-Driven Decision Making?\n",
    "\n",
    "Strengths Observed:\n",
    "\n",
    "LLMs were able to interpret well-structured prompts and return responses with reasonable accuracy for factual queries.\n",
    "\n",
    "In some cases, the LLM inferred trends or patterns (e.g., yards vs. wins), showing its capability to assist in reasoning-based analysis.\n",
    "\n",
    "Limitations Noticed:\n",
    "\n",
    "Some answers were plausible-sounding but factually wrong, especially when exact numbers or rankings were required.\n",
    "\n",
    "LLMs struggled with context that required exact matching to the dataset unless the question was very precise.\n",
    "\n",
    "The LLM's explanation quality varied, and often lacked sufficient statistical reasoning unless explicitly prompted.\n",
    "\n",
    "Usefulness in Decision Making:\n",
    "\n",
    "LLMs can augment human reasoning by offering hypotheses or first drafts of insights, but should not be used blindly.\n",
    "\n",
    "Best used as a co-pilot, where humans verify answers via EDA (Exploratory Data Analysis).\n",
    "\n",
    "They offer potential for quick insight generation, especially for hypothesis exploration or stakeholder storytelling.\n",
    "\n",
    "Recommendations:\n",
    "\n",
    "Combine LLMs with visualizations and statistical summaries for best outcomes.\n",
    "\n",
    "Use domain knowledge + verification steps to ensure outputs are valid before making decisions.\n",
    "\n",
    "Future workflows could benefit from integrating LLMs with programmatic querying (e.g., Pandas/Polars filters) for better accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b77e4e80-2c02-46fc-a18a-7ba50839f3ce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
